{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b92fc8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "client = genai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a5a6f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to your JSON file\n",
    "file_path = \"Datasets/file_analysis_2/fa2.json\"\n",
    "\n",
    "# Load the JSON content\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract parts from the JSON\n",
    "scenario = data[\"scenario\"]\n",
    "tools = data[\"tools\"]\n",
    "questions = data[\"questions\"]\n",
    "file=data[\"files\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ff39820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExecuteCmd(command_str) -> str:\n",
    "    \"\"\"Execute a shell command and save its output to a file.\n",
    "\n",
    "    Args:\n",
    "        command_str: The shell command to execute as a string.\n",
    "\n",
    "    Returns:\n",
    "        A message indicating that the output was saved in a file.\n",
    "    \"\"\"\n",
    "    output_file = 'output_cmd/output.txt'\n",
    "    with open(output_file, 'w') as f:\n",
    "        subprocess.run(\n",
    "            command_str,\n",
    "            shell=True,\n",
    "            stderr=f,\n",
    "            stdout=f,\n",
    "            text=True,\n",
    "            executable='/bin/bash'\n",
    "        )\n",
    "    return f\"Output saved in a file\"\n",
    "\n",
    "\n",
    "# Tool 1: get the next question from a list\n",
    "def next_question() -> str:\n",
    "    \"\"\"This tool reads and returns the next question and can be used to get the first question.\"\"\"\n",
    "    global i, questions\n",
    "    i += 1\n",
    "    return questions[i-1]\n",
    "\n",
    "# Tool 2: read the command output file\n",
    "def read_output() -> str:\n",
    "    \"\"\"This tool reads the file where command outputs are stored and returns its content(may be truncated if output is very long).\"\"\"\n",
    "    output_file = 'output_cmd/output.txt'\n",
    "    with open(output_file, 'r') as f:\n",
    "        output = f.read()\n",
    "    if len(output) > 500:\n",
    "        output = output[:500] + \"\\n...[truncated]...\"\n",
    "    return output.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2c4c119",
   "metadata": {},
   "outputs": [],
   "source": [
    "ExecuteCmd_dec = {\n",
    "    \"name\": \"ExecuteCmd\",\n",
    "    \"description\": \"Execute a shell command and save its output to a file.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"command_str\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The shell command to execute.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"command_str\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "next_question_dec = {\n",
    "    \"name\": \"next_question\",\n",
    "    \"description\": \"Reads and returns the next question and can be used to get the first question.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {},  # no parameters required\n",
    "        \"required\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "read_output_dec = {\n",
    "    \"name\": \"read_output\",\n",
    "    \"description\": \"Reads the file where command outputs are stored and returns its content. Output may be truncated if very long.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {},  # no parameters required\n",
    "        \"required\": []\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e73db851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are tasked with solving a cybersecurity investigation challenge composed of multiple sequential questions. Use only Ubuntu terminal commands to analyze the provided file and answer each question in order.\n",
      "\n",
      "Scenario:\n",
      "A suspicious Pwershell script in a .txt file was found on one of our endpoints, can you work out what it does?\n",
      "\n",
      "Available Command-line tools you can use:\n",
      "grep, awk, sed, sha256sum, base64, cut, sort, wc, head, tail, find, strings, file, stat\n",
      "\n",
      "Target File:\n",
      "Datasets/file_analysis_2/HDWallpaperEngine.txt \n",
      "\n",
      "You run in a loop of Thought, Action, Observation.\n",
      "At the end of the loop you output an Answer.\n",
      "Use Thought to describe your thoughts about the task you have been given.\n",
      "You must output your detailed reasoning before returning the action.\n",
      "Use Action to run one of the tools available to you using Function Calling.\n",
      "Observation will be the result of running those actions.\n",
      "\n",
      "Your available tools are:\n",
      "- ExecuteCmd\n",
      "- next_question\n",
      "- read_output\n",
      "\n",
      "Example session:\n",
      "\n",
      "Task by the user: Take the next question and solve it by providing a linux command\n",
      "\n",
      "Thought: I need to get the next question\n",
      "Action: function calling next_question\n",
      "\n",
      "You will be called again with this:\n",
      "\n",
      "Observation: the next question\n",
      "\n",
      "Thought: I need to generate a command to solve the question \n",
      "Action: Function Calling ExecuteCmd\n",
      "\n",
      "You will be called again with this:\n",
      "\n",
      "Task by the user: Analyze the previous command's output and generate a new command or return the final answer\n",
      "\n",
      "Thought: i need to see the output of the command\n",
      "Action: Function Calling read_output\n",
      "\n",
      "You will be called again with this:\n",
      "\n",
      "Observation: the output of the command\n",
      "\n",
      "Thought: I need to check if the answer is in the output, if not i will give another command \n",
      "Action: Function Calling ExecuteCmd\n",
      "OR\n",
      "Answer: answer to the question\n",
      "\n",
      "You will be called again with this:\n",
      "Task by the user: Take the next question and solve it by providing a linux command\n"
     ]
    }
   ],
   "source": [
    "prompt= f\"\"\"\n",
    "You are tasked with solving a cybersecurity investigation challenge composed of multiple sequential questions. Use only Ubuntu terminal commands to analyze the provided file and answer each question in order.\n",
    "\n",
    "Scenario:\n",
    "{scenario}\n",
    "\n",
    "Available Command-line tools you can use:\n",
    "{', '.join(tools)}\n",
    "\n",
    "Target File:\n",
    "{', '.join(file)} \n",
    "\n",
    "You run in a loop of Thought, Action, Observation.\n",
    "At the end of the loop you output an Answer.\n",
    "Use Thought to describe your thoughts about the task you have been given.\n",
    "You must output your detailed reasoning before returning the action.\n",
    "Use Action to run one of the tools available to you using Function Calling.\n",
    "Observation will be the result of running those actions.\n",
    "\n",
    "Your available tools are:\n",
    "- ExecuteCmd\n",
    "- next_question\n",
    "- read_output\n",
    "\n",
    "Example session:\n",
    "\n",
    "Task by the user: Take the next question and solve it by providing a linux command\n",
    "\n",
    "Thought: I need to get the next question\n",
    "Action: function calling next_question\n",
    "\n",
    "You will be called again with this:\n",
    "\n",
    "Observation: the next question\n",
    "\n",
    "Thought: I need to generate a command to solve the question \n",
    "Action: Function Calling ExecuteCmd\n",
    "\n",
    "You will be called again with this:\n",
    "\n",
    "Task by the user: Analyze the previous command's output and generate a new command or return the final answer\n",
    "\n",
    "Thought: i need to see the output of the command\n",
    "Action: Function Calling read_output\n",
    "\n",
    "You will be called again with this:\n",
    "\n",
    "Observation: the output of the command\n",
    "\n",
    "Thought: I need to check if the answer is in the output, if not i will give another command \n",
    "Action: Function Calling ExecuteCmd\n",
    "OR\n",
    "Answer: answer to the question\n",
    "\n",
    "You will be called again with this:\n",
    "Task by the user: Take the next question and solve it by providing a linux command \"\"\".strip()\n",
    "print (prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2f99b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3= f\"\"\"\n",
    "You are tasked with solving a cybersecurity investigation challenge composed of multiple sequential questions. Use only Ubuntu terminal commands to analyze the provided file and answer each question in order.\n",
    "\n",
    "Scenario:\n",
    "{scenario}\n",
    "\n",
    "Available Command-line tools you can use:\n",
    "{', '.join(tools)}\n",
    "\n",
    "Target File:\n",
    "{', '.join(file)} \n",
    "\n",
    "You run in a loop of Thought, Action, Observation.\n",
    "At the end of the loop you output an Answer.\n",
    "Use Thought to describe your thoughts about the task you have been given.\n",
    "You must output your detailed reasoning before returning the action.\n",
    "Use Action to run one of the tools available to you using Function Calling.\n",
    "Observation will be the result of running those actions.\n",
    "\n",
    "Your available tools are:\n",
    "- ExecuteCmd\n",
    "- next_question\n",
    "- read_output \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ce204f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2=\"\"\"You are tasked with solving a cybersecurity investigation challenge composed of multiple sequential questions. Your process must follow a strict loop: Thought, Action, Observation. At the end of the loop, you will output an Answer.\n",
    "\n",
    "**Process Rules:**\n",
    "1.  **Thought:** You MUST first describe your reasoning and plan. Explain what the goal is and which command you plan to use and why. This is a mandatory step before any action.\n",
    "2.  **PAUSE** Take a break after generating a thought\n",
    "3.  **Action:** After the Thought, you will use function calling to execute one of the available tools (dont do this if there isn't a thought in the history).\n",
    "4.  **Observation:** This will be the result of the action you took.\n",
    "\n",
    "Use only Ubuntu terminal commands to analyze the provided file and answer each question in order.\n",
    "\n",
    "**Scenario:**\n",
    "A suspicious Pwershell script in a .txt file was found on one of our endpoints, can you work out what it does?\n",
    "\n",
    "**Available Command-line tools:**\n",
    "grep, awk, sed, sha256sum, base64, cut, sort, wc, head, tail, find, strings, file, stat\n",
    "\n",
    "**Target File:**\n",
    "Datasets/file_analysis_2/HDWallpaperEngine.txt\n",
    "\n",
    "**Available Tools for Function Calling:**\n",
    "- `ExecuteCmd(command: str)`: Executes a Linux command.\n",
    "- `next_question()`: Retrieves the next question in the challenge.\n",
    "- `read_output()`: Reads the output of the last executed command.\n",
    "\n",
    "**Example Session:**\n",
    "\n",
    "**User:** Take the next question and solve it by providing a linux command.\n",
    "\n",
    "**Model:**\n",
    "Thought: The first step is to get the next question from the challenge. I will use the `next_question` function to retrieve it.\n",
    "Action: function calling next_question\n",
    "\n",
    "**User:** (Model is called again with this input, after `next_question` runs)\n",
    "Observation: the next question\n",
    "\n",
    "**Model:**\n",
    "Thought: The last step was to get the question. Now that I have it, I need to generate a command to solve it. My plan is to use a specific command on the target file.\n",
    "Action: Function Calling ExecuteCmd(command='grep -i \"some_pattern\" Datasets/file_analysis_2/HDWallpaperEngine.txt')\n",
    "\n",
    "**User:** (Model is called again with this input, after `ExecuteCmd` runs)\n",
    "Observation: the output of the command\n",
    "\n",
    "**Model:**\n",
    "Thought: I have the output of the command. I need to analyze it to see if it contains the answer. If the answer is present, I will formulate the final Answer. If not, I will need to formulate a new plan and command to get the answer.\n",
    "Action: function calling read_output()\n",
    "\n",
    "**User:** (Model is called again with this input, after `read_output()` runs)\n",
    "Observation: the output of the previous command\n",
    "\n",
    "**Model:**\n",
    "Answer: The answer to the question is '...' \"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d614be9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: next_question()\n",
      "Observation: What is the SHA256 hash value for the PowerShell script file?\n",
      "Action: ExecuteCmd(command_str='sha256sum Datasets/file_analysis_2/HDWallpaperEngine.txt')\n",
      "Observation: Output saved in a file\n",
      "Action: read_output()\n",
      "Observation: e0b7a2ad2320ac32c262aeb6fe2c6c0d75449c6e34d0d18a531157c827b9754e  Datasets/file_analysis_2/HDWallpaperEngine.txt\n",
      "The SHA256 hash value for the PowerShell script file is `e0b7a2ad2320ac32c262aeb6fe2c6c0d75449c6e34d0d18a531157c827b9754e`.\n",
      "Action: next_question()\n",
      "Observation: What email address is used to send and receive emails? (Format: Mailbox@domain.tld)\n",
      "Action: ExecuteCmd(command_str=\"grep -oE '[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}' Datasets/file_analysis_2/HDWallpaperEngine.txt\")\n",
      "Observation: Output saved in a file\n",
      "Action: read_output()\n",
      "Observation: chaudhariparth454@gmail.com\n",
      "chaudhariparth454@gmail.com\n",
      "The email address used to send and receive emails is `chaudhariparth454@gmail.com`.\n",
      "Action: next_question()\n",
      "Observation: What is the password for this email account?\n",
      "Action: ExecuteCmd(command_str='grep -iE \"(password|pass|pwd|credential)\" Datasets/file_analysis_2/HDWallpaperEngine.txt')\n",
      "Observation: Output saved in a file\n"
     ]
    }
   ],
   "source": [
    "#create a chat session\n",
    "chat = client.chats.create(\n",
    "            history=[],\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            config=types.GenerateContentConfig(\n",
    "                    # here are the external functions' declarations that the LLM will call to solve the problem\n",
    "                    tools=[types.Tool(function_declarations=[ExecuteCmd_dec,next_question_dec,read_output_dec])], \n",
    "                    system_instruction=prompt3 #system prompt\n",
    "                    \n",
    "                ))\n",
    "i=0\n",
    "answer_list=[\"answer not found\"]*len(questions)\n",
    "\n",
    "while i<len(questions):\n",
    "        \n",
    "        # we limit the yser to enter \"1\", \"2\" or \"3\" to choose what to tell the model\n",
    "        user_input = int(input(\"Enter 1 to solve the next question or 2 to further analyse the question: \"))\n",
    "        if user_input == 1:\n",
    "            query = \"Take the next question and solve it by providing a linux command always generate thought before calling\"\n",
    "            \n",
    "        elif user_input == 2:\n",
    "            query = \"Analyze the previous command's output and generate a new command or return the final answer\"\n",
    "\n",
    "        elif user_input ==3:break    \n",
    "        else:\n",
    "            print(\"Invalid input. Please enter 1 or 2.\")\n",
    "\n",
    "        #loop \n",
    "        j=0 \n",
    "        MaxAttempts=4\n",
    "        next_input=query\n",
    "        \n",
    "        while j<MaxAttempts:\n",
    "            j+=1\n",
    "            #this is the request that triggers gemini\n",
    "            response= chat.send_message(next_input)\n",
    "            parts=response.candidates[0].content.parts[0]\n",
    "            # Check for a function call\n",
    "            if parts.function_call:\n",
    "\n",
    "                tool_call = parts.function_call\n",
    "                args_str = \", \".join(f\"{k}={repr(v)}\" for k, v in tool_call.args.items())\n",
    "                print(f\"Action: {tool_call.name}({args_str})\")\n",
    "\n",
    "                if tool_call.name == \"ExecuteCmd\":\n",
    "                    result = ExecuteCmd(**tool_call.args)#there is break here\n",
    "                    print(f\"Observation: {result}\")\n",
    "                    break\n",
    "                elif tool_call.name== \"next_question\":\n",
    "                    result = next_question()\n",
    "                else: result= read_output()\n",
    "\n",
    "                next_input=f\"Observation: {result}\"\n",
    "                print(next_input)\n",
    "                \n",
    "            else:\n",
    "\n",
    "                print(parts.text)                \n",
    "                match = re.search(r\"(?i)answer\\s*:\\s*(.*)\", parts.text)\n",
    "                if match:\n",
    "                    answer_list[i-1]=match.group(1).strip()  # Extract text after 'Answer:'\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc89221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=11>\n",
      ") candidates=[Candidate(\n",
      "  content=Content(\n",
      "    parts=[\n",
      "      Part(\n",
      "        function_call=FunctionCall(\n",
      "          args={\n",
      "            'command_str': 'sha256sum Datasets/file_analysis_2/HDWallpaperEngine.txt'\n",
      "          },\n",
      "          name='ExecuteCmd'\n",
      "        ),\n",
      "        thought_signature=b'\\n\\xb7\\x01\\x01T\\xa8\\\\\\xee\\x13}\\xee\\xbbq\\xa4\\x15\\xc5\\xcf\\x1c\\x93\\x94\\xb6.f\\nP\\x82y\\xb9U\\x8a\\xd8[\\xcfBP\\x93\\xcb\\xf9\\xb4I\\x84\\xa2\\xcc\\xd2X\\xc5\\xb0\\x06\\x1b\\xd8t\\xbd\\n\\xa7eR\\xca/\\xfc\\xea\\x1b?\\xe7\\x99\\x89\\xae\\x86\\xc9\\xd1\\x02\\xc6\\xd5\\x7f\\xbf\\x97\\xb6z\\x96\\x14e\\x16>\\xce\\x05cT\\xa9s&\\x8c\\xcd\\xc4\\xd6\\x16\\xfbzo...'\n",
      "      ),\n",
      "    ],\n",
      "    role='model'\n",
      "  ),\n",
      "  finish_reason=<FinishReason.STOP: 'STOP'>,\n",
      "  index=0\n",
      ")] create_time=None response_id=None model_version='gemini-2.5-flash' prompt_feedback=None usage_metadata=GenerateContentResponseUsageMetadata(\n",
      "  candidates_token_count=34,\n",
      "  prompt_token_count=818,\n",
      "  prompt_tokens_details=[\n",
      "    ModalityTokenCount(\n",
      "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
      "      token_count=818\n",
      "    ),\n",
      "  ],\n",
      "  thoughts_token_count=48,\n",
      "  total_token_count=900\n",
      ") automatic_function_calling_history=[] parsed=None\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17354ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e0b7a2ad2320ac32c262aeb6fe2c6c0d75449c6e34d0d18a531157c827b9754e', 'chaudhariparth454@gmail.com', 'Parth@123', '587', 'user32.dll', 'temp']\n"
     ]
    }
   ],
   "source": [
    "print(answer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29747f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parts=[Part(\n",
      "  text='Take the next question and solve it by providing a linux command always generate thought before calling'\n",
      ")] role='user'\n",
      "parts=[Part(\n",
      "  text=\"I'll get the first question using `next_question` to begin the investigation.\",\n",
      "  thought_signature=b'\\n\\x93\\x01\\x01T\\xa8\\\\\\xee\\xf2\\xe1\\xe1\\xba\\xf0\\x13\\x90\\xed\\x15H\\xa7\\xd1\\xc8\\x17\\x95\\x83*\\x98T\\xff\\xfb\\xf6\\x19\\x87{\"U\\xbc\\xd1E\\xa8\\x13@\\xf8\\x0c6M~9\\xa7\\xc07C\\xf2\\xa6\\x1c\\xb3LO\\xb4\\x19\\xf1O\\xe9\\x11[]>\\xb1\\x1c\\xd9\\xe2\\xf3\\xdb\"|\\'c\\xbf8W,\\xe8D{\\xda\\x08T\\x00\\x12\\xdd<v\\xd7\\x97\\xc1\\xe80\\xe0...'\n",
      "), Part(\n",
      "  function_call=FunctionCall(\n",
      "    args={},\n",
      "    name='next_question'\n",
      "  )\n",
      ")] role='model'\n",
      "parts=[Part(\n",
      "  text='Take the next question and solve it by providing a linux command always generate thought before calling'\n",
      ")] role='user'\n",
      "parts=[Part(\n",
      "  text=\"I need to find the SHA256 hash of the file. I'll use the `sha256sum` command on the target file, `Datasets/file_analysis_2/HDWallpaperEngine.txt`, to compute the hash.\",\n",
      "  thought_signature=b'\\n\\xa4\\x03\\x01T\\xa8\\\\\\xee\\xda\\x83\\xcf\\xcbgc\\x9bLh\\x01\\xc7\\xe6 \\xc5\\x9d\\xb4JY\\xa7\\xc08\\xf6\\xc3\\x1b\\xb9_\\x06\\xff\\xa4\\xe2\\xba\\xbc\\xcc\\x9b\\x03\\x97f1\\x81b\\x08!\\x12\\xea\\xef\\xf0\\xeb\\xbf\\xfd\"\\xd0\\x88\\xa5\\xe0\\xa9E \\xd9\\x1b\\xd4x\\xd0\\xf2\\xd3\\x16\\xe6\\xd1p4\\xdbO,\\xaa)\\x7f3k\\x14\\xea\\xef\\xc9\\xaa\\xd8\\x00\\xa0z\\xc7\\xd6\\x97...'\n",
      "), Part(\n",
      "  function_call=FunctionCall(\n",
      "    args={\n",
      "      'command_str': 'sha256sum Datasets/file_analysis_2/HDWallpaperEngine.txt'\n",
      "    },\n",
      "    name='ExecuteCmd'\n",
      "  )\n",
      ")] role='model'\n",
      "parts=[Part(\n",
      "  text='Take the next question and solve it by providing a linux command always generate thought before calling'\n",
      ")] role='user'\n",
      "parts=[Part(\n",
      "  text=\"I've run the `sha256sum` command. I'll now read the output to get the hash.\",\n",
      "  thought_signature=b'\\n\\x96\\x01\\x01T\\xa8\\\\\\xee\\r\\xc2v\\xd5|4[-f\\x9cBI\\x06rq\\\\\\x94$\\xb5\\xac\\xd8\\xe2\\xb9fV\\xa5\\x04[\\xfe\\xe4\\xc3\\xdbE\\xafb?\\x8a\\xd4\\xcbMr\\x04\\x0c\\xbb\\xef\\x9e\\x1a\\x9d\\x16\\x18k\\x92dq\\x89\\x88g\\xc8\\x88\\xd3_\\xd9\\xb5\\x9c\"\\xf1\\x11\\xc6n\\x02O\\xa3\\xd6\\x00K\\xbb\\xcc[\\x0e\\xea\\xf4N%\\xb2g\\x1c\\xaa+\\xad...'\n",
      "), Part(\n",
      "  function_call=FunctionCall(\n",
      "    args={},\n",
      "    name='read_output'\n",
      "  )\n",
      ")] role='model'\n",
      "parts=[Part(\n",
      "  text='Take the next question and solve it by providing a linux command always generate thought before calling'\n",
      ")] role='user'\n",
      "parts=[Part(\n",
      "  text='9d82135502f232a51415f3a146433544d673199b5968b50f75e92543c85a213e',\n",
      "  thought_signature=b'\\n\\xad\\x02\\x01T\\xa8\\\\\\xeeZa(\\xb4\\xfe\\x9e\\xd9Y\\xe5]\\r\\x00.\\x9e\\xb9\\xbc \\xd2\\xc9vfP(\\xc2FMg\\xdd{\\x8a\\xe3?\\xbf\\xe1\\xb8\\xdb\\xb1\\x13\\x01+\\x9a|\\x8a\\x19%\\xd2-mu\\xd7\\xa5<\\x0bdN\\xbe\\xed]\\xd1\\xd1i\\x01\\x1f\"\\xcdQ\\xad78\\xd3\\xb8\\x0e\\xcc\\xd7\\xa2\\x04\\xf8\\x0c\\x8d&\\xd6gY\\x1eY\\xd7\\xe7\\xdc\\xe7...'\n",
      ")] role='model'\n"
     ]
    }
   ],
   "source": [
    "for message in chat.get_history():\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2be3c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tools = types.Tool(function_declarations=[next_question_dec])\n",
    "response=client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    \n",
    "    config = types.GenerateContentConfig(tools=[tools]),\n",
    "    contents=\"what is the whether now\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd6d9ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot tell you what the weather is right now, as I do not have access to real-time information.\n"
     ]
    }
   ],
   "source": [
    "for candidate in response.candidates:\n",
    "    for part in candidate.content.parts:\n",
    "        if hasattr(part, \"text\") and part.text:\n",
    "            print(part.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0552137f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i cannot tell you what the weather is right now, as i do not have access to real-time information.\n"
     ]
    }
   ],
   "source": [
    "print(response.candidates[0].content.parts[0].text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "94a60fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dida\n"
     ]
    }
   ],
   "source": [
    "textt=\"answer: dida\"\n",
    "match = re.search(r\"(?i)answer\\s*:\\s*(.*)\", textt)\n",
    "if match:\n",
    "    answer_text = match.group(1).strip()  # Extract text after 'Answer:'\n",
    "print(answer_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
